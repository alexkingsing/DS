{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "korean-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocomplete fix because IPYTHON UGH\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-jordan",
   "metadata": {},
   "source": [
    "# UNSUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "indonesian-cattle",
   "metadata": {},
   "source": [
    "Unlike supervised learning, unsupervirsed learning does not have the purpose of providing a prediction based on previous, labelled data.\n",
    "\n",
    "Unsupervised learning's goal is to analyze data and find \"structure\" in it or finding certain insights about the data do not require specific labeling. Examples of unsupervised learning are clusternig.\n",
    "\n",
    "Two major types of unsupervised learning: Transformation, Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-grounds",
   "metadata": {},
   "source": [
    "## TRANSFORMATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "manual-pavilion",
   "metadata": {},
   "source": [
    "Transformation are processes that extract or compute information from the provided data. For example, from simple discrete observations, you might want to obtain a KDE (kernel density curve) which is the probability that a value falls into a specific range.\n",
    "\n",
    "Transformation are very value and used in building heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-pioneer",
   "metadata": {},
   "source": [
    "### DIMENSIONALITY REDUCTION (PCA)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "breeding-cherry",
   "metadata": {},
   "source": [
    "PRINCIPAL COMPONENT ANALYSIS or PCA is an unsupervised learning algorithm that attempts to simplify and reduce the dimensions of the data by finding an approximate, simplified version of the data.\n",
    "\n",
    "PCA takes the original cloud of features and \"rotates\" it along with other features, and attempts to find a simplified, lesser-dimensional map of features.\n",
    "\n",
    "IMPORTANT: PCA attempts to preserve the VARIANCE of the features. Features are selected in order of VARIANCE until we have the number of components we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documentary-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# importing the relevant module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA is executed on the feature space BEFORE any splitting can occur, thus, train test split is not used here.\n",
    "# on principle numerical features should be normalized/ scaled before applying PCA. Categorical features should be encoded.\n",
    "\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# we instantiate the PCA and declare the number of components we want to have\n",
    "pca = PCA(n_components=6)\n",
    "# then, we fit our normalized data\n",
    "pca.fit(X_normalized)\n",
    "# finally, we create a new feature space\n",
    "\n",
    "X_final = pca.transform(X_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-awareness",
   "metadata": {},
   "source": [
    "### MANIFOLD LEARNING (MULTI DIMENSIONAL SCALING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-animal",
   "metadata": {},
   "source": [
    "MANIFOLD LEARNING, ALSO KNOWN AS MULTI-DIMENSIONAL SCALING.\n",
    "\n",
    "MDS is similar with PCA in the sense that it attempts to find a simplified, lesser-dimensional map of the provided features by creating a simpler dimensional space.\n",
    "HOWEVER, unlike PCA, MDS ATTEMPTS TO PRESERVE THE **DISTANCE** BETWEEN DATA POINTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# importing the relevant module\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# MDS is executed on the feature space BEFORE any splitting can occur, thus, train test split is not used here.\n",
    "# on principle numerical features should be normalized/ scaled before applying PCA. Categorical features should be encoded.\n",
    "\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "\n",
    "# we instantiate the mds and declare the number of components we want to have\n",
    "mds = MDS(n_components=6)\n",
    "# then, we fit our normalized data\n",
    "mds.fit(X_normalized)\n",
    "# finally, we create a new feature space\n",
    "\n",
    "X_final = mds.transform(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-skill",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "spectacular-world",
   "metadata": {},
   "source": [
    "t-SNE is very similar to MDS, except it attempts to preserve the distance between CLUSTERS of data points.\n",
    "\n",
    "t-SNE works best on datasets were there are clear patterns of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TSNE is executed on the feature space BEFORE any splitting can occur, thus, train test split is not used here.\n",
    "# on principle numerical features should be normalized/ scaled before applying PCA. Categorical features should be encoded.\n",
    "\n",
    "X_normalized = StandardScaler().fit_transform(X)\n",
    "\n",
    "tsne = TSNE(random_state = 0)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-gravity",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "noble-happiness",
   "metadata": {},
   "source": [
    "Clustering algorithms are algorithms that find groups in the data (called clusters) based on the many features existing in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-circular",
   "metadata": {},
   "source": [
    "### K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "confirmed-character",
   "metadata": {},
   "source": [
    "It's one of the most used algorithms for clustering.\n",
    "It creates clusters based on the MEAN of the values of the data.\n",
    "\n",
    "It's also one of the few algorithms that allow predicition.\n",
    "\n",
    "DRAWBACKS:\n",
    "-1  You need to know \"or randomly assign\" the number of clusters to form with the data.\n",
    "-2  Does not work with categorical data (although alternatives like K-MEDOIDS exist)\n",
    "-3  NORMALIZATION IS CRITICAL\n",
    "-4  Does not work with irregular clusters with odd shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# instantiating clustering algo with # of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# fitting the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# kmean support predict\n",
    "kmeans.predict(New_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-authority",
   "metadata": {},
   "source": [
    "### AGGLOMERATING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "flush-filling",
   "metadata": {},
   "source": [
    "These families of clusters work with a bottom-up approach.\n",
    "Basically, each data point stats as a cluster of its own and then the algorithm starts to create clusters based on the closest points.\n",
    "\n",
    "The stopping function here is the number of clusters (specified).\n",
    "\n",
    "Agglomerating is best used with HIERARCHICAL DATA.\n",
    "\n",
    "One of the best benefits of agglomerating is that allows to structure the cluster creation process with a dendogram. to create a dendogram we need to use scipy.\n",
    "\n",
    "We can also specify the linkage criteria (grouping criteria):\n",
    "- Ward: Merges the 2 clusters that create the smallest increase in variance.\n",
    "- Average: Calculates the distance based on the average distance of each point in a cluster vs every other point in the other cluster.\n",
    "- Complete / Max: Calculates the distance as the MAX distance between clusters. The clusters with the smallest MAX distance are merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clstr = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# agglomerating does not allow to predict.\n",
    "cls_assign = clstr.fit_predict(X)\n",
    "\n",
    "# to create a dendogram we need to use scipy.\n",
    "    ## read more in stackoverflow\n",
    "\n",
    "from scipy.cluster.hierarchy import ward, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-moisture",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "patent-symbol",
   "metadata": {},
   "source": [
    "Density based spatial clustering of applications with noise (DBSCANS)\n",
    "\n",
    "- Does not need a number of clusters in advance.\n",
    "- Is highly efficient and can be used for large datasets.\n",
    "- Works well with complex-shaped clusters\n",
    "\n",
    "It's key parameters are min_samples and eps.\n",
    "\n",
    "-min_samples represents the number of nearby points (including SELF) to be considered part of a cluster.\n",
    "-eps represents a sort of \"radius\" distance from a point, which is used to find the neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary module\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# instantiating\n",
    "dbscan = DBSCAN(eps=2, min_samples=2)\n",
    "\n",
    "# dbscan does not allow to predict.\n",
    "dbscan.fit_predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
