{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Works very well and fast, with properly labeled data and when the data can be used to calculate distances (numerical data)\n",
    "* Works best with small datasets \n",
    "* Much better used in **CLASIFFICATION PROBLEMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* It's very inefficient and memory heavy, so it works badly with large datasets\n",
    "* Difficult or impossible to use with highly correlated data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* K -> number fo neighbors to compare with\n",
    "* metric -> by default, this calculates Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION (INCLUDING RIDGE AND LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* As a regression model, its best used when the result is a continous value.\n",
    "* It's best used when the data is known to be linearly related\n",
    "* -RIDGE- When we have MANY features with low or medium influence.\n",
    "* -LASSO- When we have few FEATURES with medium or heavy influence. Lasso Regression tends to set 0 to the importance of features with small weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* Not use when the data is known to be not linear.\n",
    "* Not use when data cannot be normalized is a categorical in nature\n",
    "* -ordinary- has no parameters to control model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* Alpha (Ridge and Lasso) -> Regularization factor that represents the penalty given to the \"weight\" of each feature coefficient. A HIGHER ALPHA penalizes high values of weight for each future which makes the model SIMPLER; too high Alpha pushes towards underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Almost always good choice for initial exploration\n",
    "* clasiffication only\n",
    "* Easy to explain (probabilistic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* Including non-meaningful variables may throw errors. Only include the variables that are necessary and may show a correlation \n",
    "* The model should have little or no multicollinearity â€“ the independent variables should be absolutely independent of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* C -> Regularization factor. It's similar to the penalty for Ridge. HIGHER VALUES OF C mean LESS REGULARIZATION, the opposite of Alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* KSVM are very powerful in cases where the data large amount of features (dimensions) and it can overall perform well in many different datasets.\n",
    "* Can be used with linear data (Linear SVM) or with non-linear (KSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* KSVM are very limited in efficiency or processing power (large datasets are troublesome, e.g of over 50k registers)\n",
    "* KSVM are very sensitive and require extensive data normalization and tuning.\n",
    "* KSVM are very difficult to interpret as to HOW they reached an answer (unlike Logistic regression, for exmaple, which is probability based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* Kernel -> Type of kernel to use\n",
    "* C -> Regularization factors. HIGHER VALUES OF C mean LESS REGULARIZATION.\n",
    "* Y (Gamma, for RBF) -> Gamma controls the influence of a point. SMALL GAMMA means points further appart are deemed similar and the curves are smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Easy to visualize and interpret\n",
    "* No feature normalization or scaling needed\n",
    "* Works well with datasets mixing lots of variable types (numerical, nominal, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* VERY prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* max_depth -> controls the layers / lines of the tree depth. MOST COMMON CONTROLLER\n",
    "* max_leafs -> controls the max # of leafs the tree will product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* When we have high dimensional data (hundreds of features)\n",
    "* When we have sparse data\n",
    "* When we want to process text (Bernoulli / Multinomial)\n",
    "* Generally a good first try.\n",
    "* Very fast and efficient.\n",
    "* Easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* When the features are highly correlated\n",
    "* When data is not lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Does not need re-scaling or preprocessing of features\n",
    "* Excellent prediction performance on many problems\n",
    "* Can be parallelized across many CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* Very difficult to read/understand for people\n",
    "* Not GOOD for high dimensional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* n_estimator -> number of trees to generate\n",
    "* max_features  -> VERY sensitive. It's the number of features to use per tree.\n",
    "    * if 1, creates nearly unique trees every time\n",
    "    * if close to the N of features, produces nearly identical trees every time\n",
    "* max_depth -> depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTED DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Often one of the best, along with random forest, to try right away.\n",
    "* Does not require extensive parameter tuningf and can work with many types of data.\n",
    "* Very memory and runtime efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* Very difficult to read/understand for people\n",
    "* requires careful tuning of learning rate\n",
    "* training can be computationally difficult\n",
    "* NOT RECOMMENDED FOR DATA WITH MANY FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* n_estimator -> number of trees to generate. This is normally tuned first.\n",
    "* learning rate -> Strenght of the correction the algorithm attemps to apply on each tree series. When the learning rate is high, each successive tree put strong emphases on correcting the mistakes of its predecessor.\n",
    "* max_depth -> layers of the tree. Usually below 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-LAYER PERCEPTRON (MLP) NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Basis of the state of the art models\n",
    "* Capable of finding very complex relationships between data\n",
    "* An excellent choice when the features are of similar types (e.g: Pixel features of an image, dimensions of an object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* Very computationally heavy. Takes a lot of resources and time to learn\n",
    "* Very careful preprocessing is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* hidden_layers_size -> sets the number of hidden units (number inside the list) of the hidden layers (elements of the list). E.g: 100 units, 2 layers -> [100,100]\n",
    "* alpha -> Regularization factor. Higher alpha punishes for higher weight for features. Higher alpha leads to simpler models.\n",
    "* activation -> The activation function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
