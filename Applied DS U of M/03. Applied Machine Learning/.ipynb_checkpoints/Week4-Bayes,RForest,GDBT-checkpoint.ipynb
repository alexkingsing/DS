{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES, RANDOM FOREST, GRADIENT BOOSTED DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Naive bayes is a probability based classification model that makes a strong assumption about the data.\n",
    "\n",
    "Naive bayes works on the underlying assumption that the features are independent or not/have low correlated /. This allows for very fast and efficient computation but also limits the algorithms generalization ability.\n",
    "\n",
    "Naive bayes, along with logistic regression, are good -first assumption- models.\n",
    "\n",
    "Naive bayes is also very competitive in situations where there are many, many features.\n",
    "\n",
    "There are 3 types of naive bayes:\n",
    "    - Bernoulli: well-suited For binary features (Yes/No) Example: Word absence / existence in a sentence.\n",
    "    - Multinomial: well-suited for discrete features. (counts) Example: Word appearances in a paragraph.\n",
    "    - Gaussian: For continuous / real-value feature data.\n",
    "\n",
    "All 3 types of naive bayes compute Mean and STD of the feature and use this statistics, along with the distribution of the classes, to predict new values. Basically, it compares a new data point with a calculated / assumed data distribution for a feature, and it calculates the probability of the class that might have genereted the value.\n",
    "\n",
    "Naive bayes also supports partial fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Gaussian type\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# just like any other model, NB just needs to be trained with the data\n",
    "    # NB also supports partial data fit.\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random forest is an ensemble model that produces an aggregate of decision trees.\n",
    "\n",
    "Random forest is a POWERFUL classification method that generates multiple decision trees in order to get rid or minimize overfitting while mantaining prediction power. The idea is that the results will be much more easy to generalize if we average across a large number of decision trees.\n",
    "\n",
    "Random forests work by creating random variations during tree building.\n",
    "\n",
    "Randoms forests calculate the \"probability\" of choosing class by averaging the predicition of the multiple underlying trees that comprise the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# we instantiate and set the relevant parameters\n",
    "    ## estimators default is 10\n",
    "clf = RandomForestClassifier(n_estimators=12, max_features=8)\n",
    "    ## just like with any other model, once the splits are done, we just train the model.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTED DECISION TREES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GBDT are another type of ensemble, just like random forests, but in this case the model does not compute many different, random trees, but rather it creates SERIES of NON-RANDOM trees that attempt to iteratively correct the mistakes of previous trees.\n",
    "\n",
    "GBDT use \"shallow\" or simple trees to try to correct the mistakes of previous trees. Normally, GBDT have a very short depth, between 2 or 4 layers of depth. Once the model has been built, GBDT are very memory efficient and effective.\n",
    "\n",
    "GBDT have a key parameter called \"Learning rate\". The learning rate controls the rate at which the series of trees attempt to correct the mistakes of previous trees. The learning rate controls how the gradient boost the tree algorithms, builds a series of collective trees. When the learning rate is high, each successive tree put strong emphases on correcting the mistakes of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the relevant class \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# we instantiate and set the relevant parameters\n",
    "    ## learning rate is the key parameter to tune. Default value is 0.1.\n",
    "    ## n_estimators is set to 100.\n",
    "    ## max_depth default is 3.\n",
    "clf = GradientBoostingClassifier()\n",
    "    ## just like with any other model, once the splits are done, we just train the model.\n",
    "clf.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
