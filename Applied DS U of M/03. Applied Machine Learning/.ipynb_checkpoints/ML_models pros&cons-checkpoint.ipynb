{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Works very well and fast, with properly labeled data and when the data can be used to calculate distances (numerical data)\n",
    "* Works best with small datasets \n",
    "* Much better used in **CLASIFFICATION PROBLEMS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* It's very inefficient and memory heavy, so it works badly with large datasets\n",
    "* Difficult or impossible to use with highly correlated data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* K -> number fo neighbors to compare with\n",
    "* metric -> by default, this calculates Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION (INCLUDING RIDGE AND LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* As a regression model, its best used when the result is a continous value.\n",
    "* It's best used when the data is known to be linearly related\n",
    "* -RIDGE- When we have MANY features with low or medium influence.\n",
    "* -LASSO- When we have few FEATURES with medium or heavy influence. Lasso Regression tends to set 0 to the importance of features with small weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* Not use when the data is known to be not linear.\n",
    "* Not use when data cannot be normalized is a categorical in nature\n",
    "* -ordinary- has no parameters to control model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* Alpha (Ridge and Lasso) -> Regularization factor that represents the penalty given to the \"weight\" of each feature coefficient. A HIGHER ALPHA penalizes high values of weight for each future which makes the model SIMPLER; too high Alpha pushes towards underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Almost always good choice for initial exploration\n",
    "* clasiffication only\n",
    "* Easy to explain (probabilistic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when not to use\n",
    "* Including non-meaningful variables may throw errors. Only include the variables that are necessary and may show a correlation \n",
    "* The model should have little or no multicollinearity â€“ the independent variables should be absolutely independent of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "* C -> Regularization factor. It's similar to the penalty for Ridge. HIGHER VALUES OF C mean LESS REGULARIZATION, the opposite of Alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* KSVM are very powerful in cases where the data large amount of features (dimensions) and it can overall perform well in many different datasets.\n",
    "* Can be used with linear data (Linear SVM) or with non-linear (KSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* KSVM are very limited in efficiency or processing power (large datasets are troublesome, e.g of over 50k registers)\n",
    "* KSVM are very sensitive and require extensive data normalization and tuning.\n",
    "* KSVM are very difficult to interpret as to HOW they reached an answer (unlike Logistic regression, for exmaple, which is probability based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* Kernel -> Type of kernel to use\n",
    "* C -> Regularization factors. HIGHER VALUES OF C mean LESS REGULARIZATION.\n",
    "* Y (Gamma, for RBF) -> Gamma controls the influence of a point. SMALL GAMMA means points further appart are deemed similar and the curves are smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros / when to use\n",
    "* Easy to visualize and interpret\n",
    "* No feature normalization or scaling needed\n",
    "* Works well with datasets mixing lots of variable types (numerical, nominal, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons / when NOT to use\n",
    "* VERY prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Parameters\n",
    "* max_depth -> controls the layers / lines of the tree depth. MOST COMMON CONTROLLER\n",
    "* max_leafs -> controls the max # of leafs the tree will product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
